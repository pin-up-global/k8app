# Default values for agent.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
info:
  name: agent
  license: PLZ_CHANGE
  project: PLZ_CHANGE
  deployment_environment: PLZ_CHANGE

IsDaemonSet: false

gateway:
  addr: 127.0.0.1:4317

replicaCount: 1

service:
  type: ClusterIP
  ports:
    - port: 4317
      name: otelgrpc
    - port: 4318
      name: otelhttp

rbac:
  create: true

podSecurityPolicy:
  enabled: false

image:
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.88.0"

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "collector"
  # Annotations to add to the service account
  annotations: { }

podAnnotations: { }

podSecurityContext: { }
# fsGroup: 2000

securityContext: { }
# capabilities:
#   drop:
#   - ALL
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000

affinity: {}
nodeSelector: {}
resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 0.5Gi
tolerations: []

# required AWS SSM parameter store certificates
certs:
  "ca.crt": "ca.crt"
  "client.crt": "client.crt"
  "client.key": "client.key"


config:
  receivers:
    otlp:
      protocols:
        http: {}
        grpc:
          max_recv_msg_size_mib: 32
          max_concurrent_streams: 16
    filelog:
      include:
        - /var/log/pods/*/*/*.log
      exclude:
        # Exclude logs from all containers named otel-collector
        - /var/log/pods/*/otel-collector/*.log
      start_at: end
      include_file_path: true
      include_file_name: false
      operators:
        # Find out which format is used by kubernetes
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # Parse CRI-O format
        - type: regex_parser
          id: parser-crio
          regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: '2006-01-02T15:04:05.000000000-07:00'
        # Parse CRI-Containerd format
        - type: regex_parser
          id: parser-containerd
          regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        # Parse Docker format
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
        # Rename attributes
        - type: move
          from: attributes.log
          to: body
        - type: move
          from: attributes.stream
          to: attributes["log.iostream"]
        - type: move
          from: attributes.container_name
          to: resource["k8s.container.name"]
        - type: move
          from: attributes.namespace
          to: resource["k8s.namespace.name"]
        - type: move
          from: attributes.pod_name
          to: resource["k8s.pod.name"]
        - type: move
          from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
        - type: move
          from: attributes.uid
          to: resource["k8s.pod.uid"]
  exporters:
    prometheus:
      endpoint: "0.0.0.0:9000"
      send_timestamps: true
      metric_expiration: 60m
      resource_to_telemetry_conversion:
        enabled: true
    logging:
      # detailed|normal|basic
      verbosity: basic
      sampling_initial: 10
      sampling_thereafter: 50
    otlp/global:
      endpoint: "{{ .Values.gateway.addr }}"
      compression: snappy
      tls:
        ca_file: /certs/ca.crt
        cert_file: /certs/client.crt
        key_file: /certs/client.key
      sending_queue:
        enabled: true
        num_consumers: 20
        queue_size: 1000
      retry_on_failure:
        enabled: false
        initial_interval: 1s
        max_interval: 10s
        max_elapsed_time: 20s
  processors:
    transform/metrics:
      metric_statements:
        - context: resource
          statements:
            # fix duplication: replace 127.0.0.1:8888 => host.name value
            - set(attributes["service.instance.id"], attributes["host.name"]) where attributes["service.name"] == "{{ .Values.info.name }}"
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      extract:
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
      # Pod association using resource attributes and connection
      pod_association:
        - sources:
            - from: resource_attribute
              name: ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: host.name
        - sources:
            - from: connection
              name: ip
    transform:
      metric_statements:
        - context: resource
          statements:
            - delete_key(attributes, "k8s.pod.uid")
            - delete_key(attributes, "k8s.container.restart_count")
            - delete_key(attributes, "k8s.container.name")
            - replace_all_patterns(attributes, "key", "k8s.node.name", "nodename")
      log_statements:
        - context: resource
          statements:
            - delete_key(attributes, "k8s.pod.uid")
            - delete_key(attributes, "k8s.container.restart_count")
            - replace_all_patterns(attributes, "key", "k8s.container.name", "container")
            - replace_all_patterns(attributes, "key", "k8s.namespace.name", "namespace")
            - replace_all_patterns(attributes, "key", "k8s.pod.name", "pod")
        - context: log
          statements:
            - delete_key(attributes, "logtag")
            - delete_key(attributes, "log.file.path")
    resource/tempo:
      attributes:
        - key: "service.name"
          from_attribute: "service"
          action: update
        - key: "service"
          action: delete
    attributes/loki:
      actions:
        - action: insert
          key: loki.attribute.labels
          value: level
        - action: delete
          key: time
    resource/loki-labels:
      attributes:
        - action: insert
          key: loki.resource.labels
          value: >-
            aws_region,
            container,
            deployment_environment,
            license,
            namespace,
            pod,
            project,
            service
    resource/loki-format-logfmt:
      attributes:
        - action: insert
          key: loki.format
          value: logfmt
    resource/loki-format-raw:
      attributes:
        - action: insert
          key: loki.format
          value: raw
    filter/non-container:
      error_mode: ignore
      logs:
        exclude:
          match_type: regexp
          resource_attributes:
            - key: k8s.container.name
              value: "^.+$"
    filter/container:
      error_mode: ignore
      logs:
        include:
          match_type: regexp
          resource_attributes:
            - key: k8s.container.name
              value: "^.+$"
    # must be after transform always!!!!
    resource:
      attributes:
        - key: project
          value: "{{ .Values.info.project }}"
          action: upsert
        - key: license
          action: upsert
          value: "{{ .Values.info.license }}"
        - key: deployment_environment
          value: "{{ .Values.info.deployment_environment }}"
          action: upsert
        - key: cluster
          value: "{{ .Values.info.project }}/{{ .Values.info.license }}/{{ .Values.info.deployment_environment }}"
          action: upsert
        ### to the end after namespace declaration
        - action: upsert
          from_attribute: service
          key: service.name # required for proper job name of target_info
        - action: upsert
          from_attribute: project
          key: service.namespace
        - key: http.scheme
          action: delete
        - key: net.host.port
          action: delete
        # bugfix: is already exist namespace - we mustn't replace it
        # previously was in transform/metric_statements and overwrite some important system metrics from k8s
        - key: namespace
          action: update
          from_attribute: k8s.namespace.name
        - key: k8s.namespace.name
          action: delete
        # bugfix: was replace existed system pod matrix when it was already existed liable, this cased creation label pod_export
        # previously it was done by replace in prometheus job
        - key: namespace
          action: update
          from_attribute: k8s.pod.name
        - key: k8s.pod.name
          action: delete
    batch:
      send_batch_size: 500
      send_batch_max_size: 1000
      timeout: 15s
    batch/metrics:
      send_batch_size: 10000
      send_batch_max_size: 20000
      timeout: 15s
    # Enabling the memory_limiter is strongly recommended for every pipeline.
    # Configuration is based on the amount of memory allocated to the collector.
    # The configuration below assumes 2GB of memory. In general, the ballast
    # should be set to 1/3 of the collector's memory, the limit should be 90% of
    # the collector's memory up to 2GB, and the spike should be 25% of the
    # collector's memory up to 2GB. In addition, the "--mem-ballast-size-mib" CLI
    # flag must be set to the same value as the "ballast_size_mib". For more
    # information, see
    # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md
    memory_limiter:
      check_interval: 2s
      limit_mib: 6000
      spike_limit_mib: 1000
  extensions:
    health_check: {}
    pprof:
      endpoint: :1888
    zpages:
      endpoint: :55679
  service:
    extensions: [ pprof, zpages, health_check ]
    telemetry:
      logs:
        level: INFO
    pipelines:
      traces:
        receivers: [ otlp ]
        processors: [ memory_limiter, resource, resource/tempo, batch ]
        exporters: [ logging, otlp/global ]
      metrics:
        exporters: [ logging, otlp/global ]
        processors:
          - memory_limiter
          - transform
          - resource
          - transform/metrics
          - batch/metrics
        receivers: [ otlp, prometheus ]
      logs/logfmt: &logs-pipeline
        exporters: [ logging, otlp/global ]
        processors:
          - memory_limiter
          - filter/non-container
          - resource/loki-format-logfmt
          - attributes/loki
          - k8sattributes
          - transform
          - resource
          - resource/loki-labels
          - batch
        receivers: [ otlp, filelog ]
      logs/raw:
        <<: *logs-pipeline
        processors:
          - memory_limiter
          - filter/container
          - resource/loki-format-raw
          - attributes/loki
          - k8sattributes
          - transform
          - resource
          - resource/loki-labels
          - batch

prometheus:
    default:
      enabled: true
      # good documentation related to SD: https://docs.victoriametrics.com/sd_configs.html#kubernetes_sd_configs
      scrape_configs: []
    append:
      enabled: false
      scrape_configs: []
    kubernetes:
      enabled: true
      scrape_configs:
        # coredns
        # https://grafana.com/grafana/dashboards/14981-coredns/
        # dashboard ID: 14981
        # https://grafana.com/grafana/dashboards/15762-kubernetes-system-coredns/
        # dashboard ID: 15762
        - job_name: coredns
          honor_labels: false
          kubernetes_sd_configs: &sd-pod
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          relabel_configs:
            - action: keep
              source_labels: # k8s-app: kube-dns
                - __meta_kubernetes_pod_label_k8s_app
                - __meta_kubernetes_pod_labelpresent_k8s_app
              regex: (kube-dns);true
            - source_labels: [ __address__ ]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?
              replacement: $$1:9153
            - action: drop
              source_labels:
                - __meta_kubernetes_pod_phase
              regex: (Failed|Succeeded)
            - action: replace
              replacement: coredns
              target_label: job_name
        - job_name: 'kubernetes-kubelet'
          scrape_interval: 1m
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
            - role: node
          relabel_configs:
            - target_label: __address__
              replacement: kubernetes.default.svc.cluster.local:443
            - source_labels: [ __meta_kubernetes_node_name ]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$$1/proxy/metrics
            - action: replace
              replacement: kubernetes-kubelet
              target_label: job_name
        # search pods containing prometheus.io/scrape annotation
        #
        # example:
        # prometheus.io/path: /metrics
        # prometheus.io/port: "7777"
        # prometheus.io/scrape: "true"
        # prometheus.io/param: "xxx=yyy"
        # prometheus.io/label.key: "value"
        #
        # example:
        #  prometheus.io/label.origin_prometheus: "observability/com/dev"
        # use for self-monitoring
        # dashboard: ID: 17728 https://grafana.com/grafana/dashboards/17728-opentelemetry-collector-0-68-0-v20221229/
        # https://grafana.com/grafana/dashboards/13332-kube-state-metrics-v2/
        # dashboard ID: 13332
        - job_name: kubernetes-pods
          kubernetes_sd_configs: *sd-pod
          honor_labels: false
          scrape_interval: 15s
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            # SRE-26 Allow to add static labels to pod metrics via pod annotations
            # Example:
            #   Annotation on pod "prometheus.io/label-custom-label: custom-label-value"
            #   will be added to a metrics produced by the bod as label custom_label with value custom-label-value
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: drop
              regex: Pending|Succeeded|Failed|Completed
              source_labels:
                - __meta_kubernetes_pod_phase
            - action: replace
              source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
              target_label: component
            - action: replace
              source_labels: [__meta_kubernetes_pod_container_name]
              target_label: container
            - action: replace
              replacement: "$$1"
              source_labels: [__meta_kubernetes_pod_container_name]
              target_label: job
            - action: replace
              separator: "/"
              source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_pod_label_app_kubernetes_io_component]
              target_label: job
            - action: replace
              replacement: kubernetes-pods
              target_label: job_name
        #[dashboard:ID: 15661](https://grafana.com/grafana/dashboards/15661-1-k8s-for-prometheus-dashboard-20211010/)
        - job_name: cadvisor # daemonset_cadvisor kubernetes-cadvisor
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs: [role: node]
          metric_relabel_configs:
          - action: keep
            source_labels: [__name__]
            regex: container_cpu_usage_seconds_total|container_fs_limit_bytes|container_fs_usage_bytes|container_fs_writes_bytes_total|container_memory_rss|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_transmit_bytes_total|container_spec_cpu_period|container_spec_cpu_quota|container_spec_memory_limit_bytes|target_info
          # Drop cgroup metrics with no pod.
          - action: drop
            regex: '.+;'
            source_labels: [id, pod]
          # Drop id label
          - action: labeldrop
            regex: ^id$
          relabel_configs:
          - replacement: kubernetes.default.svc.cluster.local:443
            target_label: __address__
          - regex: (.+)
            replacement: /api/v1/nodes/$$1/proxy/metrics/cadvisor
            source_labels: [__meta_kubernetes_node_name]
            target_label: __metrics_path__
          # Add lables
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_node_name]
            target_label: node
          - source_labels: [__meta_kubernetes_node_name]
            target_label: nodename
          - action: replace
            replacement: cadvisor
            target_label: job_name
          scheme: https
          scrape_interval: 1m
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
